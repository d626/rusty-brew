\section{Implementasjon}
\todo[inline]{Et kapittel med implementasjon, konkret hvordan er systemet laget med begrunnete valg. Forklar bruk av verktøy.}

Når det gjalt valg av maskinvare var det to muligheter. Den ene var å bruke en mikrokontroller for å lese fra sensorene, og styre pådragsorganene, og å ha en sepparat PC som kjørte serveren. Den andre var å gjøre alt fra en enkelt Raspberry Pi. Valget falt på å bruke Raspberry Pi, fordi man da slapp å jobbe med to ulike datamaskiner, det meste av utviklingen og testingen kunne skje på en hvilken som helst PC med Linux og i tillegg er Rust for embedded ikke veldig modent.

Når valet om å bruke filer for å lagre loggene var implementasjonen av log-modulen nokså enkel. Modulen har en datatype for en log, og en ``logger''. Loggerens oppgave er å abstrahere vekk den underliggende implementasjonen av log-systemet. Loggeren brukes mens loggen lages, i tillegg er det noen funksjoner som gir mulighet for å opperere på eksisterende logger.

Serveren ble implementert ved hjelp av rammeverket Rocket, ett av de største rammeverkene for webutvikling i Rust. Det bruker mye kodegenerering, og brukeren av rammeverket trenger i utgangspunktet bare å implementere funksjonene som kjøres når det kommer en forespørsel til en gitt sti. Ett annet alternativ er Iron, som er litt mindre, men det ble valgt vekk grunnet dårlig erfaring fra tidligere. Man kunne i tillegg ha lagd serveren selv ved hjelp av biblioteker, men det hadde blitt mye arbeid, for liten nytte. Rocket viste seg å være ganske lett å bruke. Det drar nytte av Rust sitt typesystem på en god måte, noe som gjør at brukeren av rammeverket bare trenger å si hvordan en type skal hentes ut fra en forespørsel, eller konverteres til en respons. Alt det andre av infrastruktur tar rammeverket seg av. På grunn av en antakelse om at Rocket hadde implementert en fornuftig måte å konvertere fra \texttt{std::io::Result}\footnote{Rust sin type for å håndtere mulige feil som har med fil-io å gjøre.} til \texttt{rocket::Response} ble det ikke implementert en egen \texttt{Result}-type. Under testingen viste det seg at dette fører til at brukeren bare får en HTTP-500 respons hvis noe går galt på serveren, selv om det ofte skyldes dårlig bruker-innput. Dette er en svakhet med den nåverende implementasjonen, fordi klienten ikke har noen mulighet til å si noe om hvorfor operasjonen feilet til sin bruker. Det er derimot ikke veldig vanskelig å endre dette i etterkant, men på grunn av andre prosjekter har denne forbedringen blitt prioritert ned. Rocket bruker naturlig nok mange tråder for å hpndtere ulike forespørsler, noe som fører til at objekter som kan brukes av flere forespørsler samtidig må være laget på en slik måte at de trygt kan deles mellom flere tråder. Dette la en del begrensninger på \texttt{Controller}-typen, som blir brukt både for å starte en ny prosess og å hente ut nåverende prosessverdier.

Ett stort problem her er at \texttt{Controller}-objekter inneholder objekter som implementerer \texttt{Input} og \texttt{Output}. Fordi man ikke vet hvilken type input og pådragsorgan man har må man bruke en form for generisk kode. Alle \texttt{Controller}-objektene legges i ett HashMap, og derfor må alle \texttt{Controller}-objektene ha samme type (og inneholde verdier av samme type). En mulig løsning er at \texttt{Controller}-objektene inneholder en referanse til innputt- og utputt-objektene. Her klager derimot Rust, fordi den ikke klarer å bevis at innputt- og utputt-objektene lever lenge nok. I tillegg må det være mulig å få tak i mutable referanser til utputt-objektet fra en annen tråd. Resultatet var å lagre en peker til ett heap-allokert objekt som må leve i hele programmets levetid, inni en atomisk referanse-tellende mutex. Dette sikrer den nødvendige synkroniseringen av tråder, samtidig som flere tråder får tilgang til objektet og de kan være sikre på at det ikke blir deallokert.

Den andre store utfordringen som ble møtt i løpet av implementeringen var hvordan infrastrukturen rundt selve reguleringen skulle være. Fordi API-et ikke kan blokkere er \texttt{start\_controlling} funksjonen nødt til å starte en ny tråd. Dette gjøres i \texttt{Controller}-objektets \texttt{start}-metode. Samtidig er det noe opprydning som må gjøres når prosessen er ferdig, og det hadde derfor vært ønskelig å vente helt til det skjer. Løsningen jeg endte opp med var å lage en tråd, som igjen lager de trådene som trengs for å faktisk gjøre reguleringen. Den første tråden venter så til de indre trådene er ferdige, før den gjør opprydningen, og selv gjør seg ferdig. For selve reguleringen blir det brukt tre tråder: en for å holde styr på hvor langt man har kommet i prosessforløpet, en som gjør selve reguleringen, og en som sørger for at de to andre trådene kjører med ett fast intervall. Disse tre trådene bruker kanaler for å kommunisere.

I tillegg til de to overnevnte utfordringene viste det seg å være mer problematisk å lese temperaturen fra temperatursensoren enn først antatt. I Rust finnes det ett maskinvare-uavhengig abstraksjonslag som gir tilgang til funksjonalitet som io-pinner og delays. Store deler av dette abstraksjonslaget er implementert for Linux, og det finnes ett bibliotek for den temperatursensoren som blir brukt i prosjektet som bruker dette abstraksjonslaget. Når dette biblioteket ble testet viste det seg derimot at det forsøkte å ``bit-bange'' onewire-protokollen. Dette viste seg å gå for tregt i en Linux user-space prosess til at det klarte å overholde timing-kravene til bussen. I stedet for dette biblioteket brukes derfor Linux sin kernel-modul for onewire. Denne modulen representerer hver sensor som en fil som man kan lese fra for å få den målte temperaturen.